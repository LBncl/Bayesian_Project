---
title: "Project Report - MAS8405"
author: "Lloyd Bates"
date: "25/02/2022"
output: pdf_document
---

```{r load, warning=TRUE, include=FALSE}
# Libraries used
library(ProjectTemplate)
library(tidyverse)
library(rjags)
library(coda)
library(ggplot2)
# Please set the working directory to the root folder "Bayesian_Analysis"
setwd("C:/Users/Lloyd Bates/Desktop/Postgraduate Newcastle Data Science MSc/Bayesian Data Analysis/Project/Bayesian_Analysis")
load.project()
```

# Data Description
## Original Data
```{r originalData}
# First few entries of data
head(as.data.frame(Reisby))
```
The data set contains 250 observations of 7 variables: "id","hd","week","lnimi" "lndmi","female","reactive_depression". Here one of the main variables that we will be observing is "lnimi" which represents the log concentration the antidepressant drug Imipramine (IMI) in a patients blood. The question of this report is observe the effectiveness of this drug on a patients depression.

## Data Preprocessing
### 01-A File
Before analysis can be completed it is important to format the data correctly, one detail about the data set is that each row is not a unique person in total there are:  
```{r groups}
# Number of different patients
groups = unique(reisby$id)
length(groups)

# Re-encode id vector
reisby$id = match((reisby$id), groups)
```
This number means that each person was not measured every week. As well the id column has been modified to go from 1:66. Further data transformations have taken place including: converting the week column to a 0/1 representing a placebo week or a week where the drug was administered, also, the Hamilton depression index has been encoded to show the 4 possible levels of depression. Finally, for both a normalized and raw format, the data as been split into test and train data. Note groups are not split as train[200] is 53 and test[201] = 54

```{r newData}
head(reisby)
```

Next the structure of the data frame can be viewed to see if how the variables are stored: 

```{r str}
# View how data is stored
str(reisby)
```

# Exploritory Data Analysis
Firstly to view any relationships between the variables the sactterplotmattrix can be viewed: 

```{r pairs}
# View scatterplot matrix
pairs(reisby)
```

# Regression to Predict Efficacy of Imipramine
Fit a multiple linear regression to the reisby data set and report the posterior mean and a 95% HPD interval for each parameter. To test the model different priors will be used with both a standardized and raw version of the data. We will use the lnimi as the response.

## Jags Code
For the first multiple linear regression the JAGS code is: 

```{r JAGS1}
modelstring="
  model {
    k = 10^3
    b0 ~ dnorm(0, k)
    
    for (j in 1:p) {
      b[j] ~ dnorm(0, k)
    }
    
    tau ~ dgamma(0.001, 0.001)
    sd = pow(tau, -0.5)
    
    for (i in 1:N) {
      y[i] ~ dnorm(mu[i], tau)
      mu[i] = b0 + inprod(b, x[i,])
    }
}"
```

### Prior Selection
For the first model the prior used will be a uninformed, normal distribution as to make bâ€™s to be almost flat.

## R Code
```{r code1}
y = reisby$lndmi
x = reisby[,-4]
x = x[,-1]

jags_data = list(y=y, x=x, N=nrow(x), p=ncol(x))
model = jags.model(textConnection(modelstring), data=jags_data, n.chains=4)
update(model, n.iter=1000)
samples = coda.samples(model, variable.names=c("b0", "sd", "b"),n.iter=1000)
```

## MCMC Diagnostics
Note for this first model only will the diagnostics be shown, it can be assumed that in all following models the same diagnostics will be used.

```{r diag1}
# ACF
mcmc_mat = as.matrix(samples[[1]])
par(mfrow=c(3,3))
for (i in 1:7) {
  acf(mcmc_mat[,i], lag.max=60, main=colnames(mcmc_mat)[i])
}
crosscorr.plot(samples)
```

From the acf plots it is clear that lag 30 is still significant, so a thinning interval of around 32 should be sufficient, now the new JAGS code will look like: 

```{r JAGS2}
jags_data = list(y=y, x=x, N=nrow(x), p=ncol(x))
model = jags.model(textConnection(modelstring), data=jags_data, n.chains=4)
update(model, n.iter=1000)
th = 35
samples = coda.samples(model,
                       variable.names=c("b0", "sd", "b"), thin=th,
                       n.iter=th*1000)

# View how well the chains mixed
gelman.diag(samples, multivariate = FALSE)

# Trace 
par(mfrow=c(3,3))
plot(samples, auto.layout=FALSE, density=FALSE)
```

## Summary
From the mcmc output the following summaries can be found:

```{r DIC1, echo=FALSE}
dic.samples(model, n.iter=th*1000, thin=th)
mcmc_mean = summary(samples)$statistics[,1]
table = as.data.frame(HPDinterval(samples[[1]])[1:5,])
table['mean'] = mcmc_mean[1:5]
table
```

Posterior density plots based on MCMC output:
These show that the posterior distributions are approximately normal.

```{r density1}
par(mfrow=c(2,3))
for (i in 1:5) {
    d = density(mcmc_mat[,i]) 
    plot(d, main=colnames(mcmc_mat)[i])
}
```

## Standarized Data
To try help improve the performance of our model the data will be scaled using the the scale() function which will transform the data so that the standard deviation is 1 and the mean is 0. The prior distribution and the JAGS and R code required to run the analysis are the same as the previous model.

```{r stanardized, echo=FALSE}
y = as.numeric(scale(reisby$lndmi))
x = reisby[,-4]
x = x[,-1]
x[3] = scale(x[3])

jags_data = list(y=y, x=x, N=nrow(x), p=ncol(x))
model = jags.model(textConnection(modelstring), data=jags_data, n.chains=4)
update(model, n.iter=1000)
th = 32
samples = coda.samples(model,
                       variable.names=c("b0", "sd", "b"), thin=th,
                       n.iter=th*1000)
```

We can view how well this new model fits the data:

```{r gelmanPlot}
gelman.plot(samples)
```

## Summary
From the mcmc output the following summaries can be found:
```{r DIC2, echo=FALSE}
dic.samples(model, n.iter=th*1000, thin=th)
mcmc_mean = summary(samples)$statistics[,1]
table = as.data.frame(HPDinterval(samples[[1]])[1:5,])
table['mean'] = mcmc_mean[1:5]
table
```

Posterior density plots based on MCMC output:
These show that the posterior distributions are approximately normal.

```{r density2, echo=FALSE}
par(mfrow=c(2,3))
for (i in 1:5) {
    d = density(mcmc_mat[,i]) 
    plot(d, main=colnames(mcmc_mat)[i])
}
```
Only differences in DIC are important, its absolute size is irrelevant so we can clearly see that the model with the standardized data offers no greater level of statistical accuracy. There is still no clear agreement on what constitutes a considerable difference in DIC but single digit differences can be easily disregarded as important.

## Differing Priors
Since the MCMC diagnostics showed no difference between standardized and raw data one other way to try improve the statistical model is to try different prior distributions, the half-Cauchy allows extreme values to occur
more frequently which may allow the model to cope better with some of the values which are further from the mean. Note the standarized data will be used for this new prior.

## JAGS Code
```{r JAGS3, include=FALSE}
modelstring="
  model {
  b0 ~ dnorm(0, 1E-6)
  for (j in 1:p) {
    b[j] ~ dnorm(0, 1E-6)
  }
  tau ~ dt(0,1/9,1) T(0,)
  sd = pow(tau, -0.5)
  for (i in 1:N) {
    y[i] ~ dnorm(mu[i], tau)
  mu[i] = b0 + inprod(b, x[i,])
  }
}"
```

## R Code
```{r code3, include=FALSE}
y = as.numeric(scale(reisby$lndmi))
x = reisby[,-4]
x = x[,-1]
x[3] = scale(x[3])

jags_data = list(y=y, x=x, N=nrow(x), p=ncol(x))
model = jags.model(textConnection(modelstring), data=jags_data, n.chains=4)
update(model, n.iter=1000)
th = 20
samples = coda.samples(model,
                       variable.names=c("b0", "b"), thin=th,
                       n.iter=th*1000)
```

## Summary
From the mcmc output the following summaries can be found:
```{r DIC3, echo=FALSE}
dic.samples(model, n.iter=th*1000, thin=th)
mcmc_mean = summary(samples)$statistics[,1]
table = as.data.frame(HPDinterval(samples[[1]])[1:5,])
table['mean'] = mcmc_mean[1:5]
table
```

```{r density3, echo=FALSE}
par(mfrow=c(2,3))
for (i in 1:5) {
    d = density(mcmc_mat[,i]) 
    plot(d, main=colnames(mcmc_mat)[i])
}
```

## Intrepretation
With all of the models retrieving negative means for some of the parameters this could be explained by having a lower starting concentration of the precursor(lnimi) means a lower concentration of lndmi in the blood.